{
  "schema_type": "tech_spec",
  "id": "SPEC-0027_dat-large-file-streaming",
  "title": "DAT Large File Streaming Implementation Specification",
  "status": "draft",
  "version": "1.0.0",
  "created_at": "2025-12-28",
  "updated_at": "2025-12-28",
  "author": "Mycahya Eggleston",
  "scope": "subsystem:DAT",
  "implements_adrs": [
    "ADR-0040_Large-File-Streaming-Strategy",
    "ADR-0013_Cancellation-Semantics-Parse-Export"
  ],
  "tier_0_contracts": [
    {
      "module": "shared.contracts.dat.adapter",
      "classes": [
        "StreamOptions",
        "StreamChunk"
      ]
    },
    {
      "module": "shared.contracts.dat.cancellation",
      "classes": [
        "Checkpoint",
        "CheckpointRegistry",
        "CancellableOperation"
      ]
    }
  ],
  "overview": "This specification defines how DAT handles large files (> 10MB) using streaming and chunked processing. The 10MB threshold determines when to switch from eager loading to streaming mode. Streaming operations support cancellation with checkpoint preservation, ensuring no data loss on interruption.",
  "requirements": [
    {
      "id": "REQ-STREAM-001",
      "description": "Files > 10MB MUST use streaming mode automatically",
      "priority": "must",
      "acceptance_criteria": [
        "File size check before processing",
        "Automatic mode selection based on threshold",
        "Configuration override available for testing"
      ]
    },
    {
      "id": "REQ-STREAM-002",
      "description": "Streaming MUST use configurable chunk sizes",
      "priority": "must",
      "acceptance_criteria": [
        "Default chunk size: 50,000 rows",
        "Configurable via StreamOptions.chunk_size_rows",
        "Chunk size adjustable based on available memory"
      ]
    },
    {
      "id": "REQ-STREAM-003",
      "description": "Memory usage MUST stay within configured limits",
      "priority": "must",
      "acceptance_criteria": [
        "Default max_memory_mb: 200MB",
        "Configurable via StreamOptions.max_memory_mb",
        "Graceful handling when limit approached"
      ]
    },
    {
      "id": "REQ-STREAM-004",
      "description": "Progress updates MUST be emitted during streaming",
      "priority": "must",
      "acceptance_criteria": [
        "Progress callback invoked after each chunk",
        "Percentage calculation based on bytes processed",
        "WebSocket updates for long-running operations"
      ]
    },
    {
      "id": "REQ-STREAM-005",
      "description": "Streaming MUST support cancellation with checkpoint preservation",
      "priority": "must",
      "acceptance_criteria": [
        "Cancel token checked between chunks",
        "Completed chunks preserved on cancellation",
        "Partial data from current chunk discarded"
      ]
    },
    {
      "id": "REQ-STREAM-006",
      "description": "Schema probing MUST complete in < 5 seconds regardless of file size",
      "priority": "must",
      "acceptance_criteria": [
        "Only first N rows read for schema inference",
        "Default: 1000 rows for schema inference",
        "Timeout enforcement"
      ]
    },
    {
      "id": "REQ-STREAM-007",
      "description": "Preview for large files MUST use sampling",
      "priority": "must",
      "acceptance_criteria": [
        "Files 10-100MB: 10,000 row preview",
        "Files 100MB-1GB: 5,000 row preview",
        "Files > 1GB: 1,000 row preview"
      ]
    }
  ],
  "implementation_details": {
    "file_size_tiers": [
      {
        "tier": "small",
        "size_range_mb": [
          0,
          0.1
        ],
        "strategy": "eager_load",
        "preview_rows": "all",
        "memory_multiplier": 10
      },
      {
        "tier": "medium",
        "size_range_mb": [
          0.1,
          10
        ],
        "strategy": "eager_load_with_progress",
        "preview_rows": "all",
        "memory_multiplier": 5
      },
      {
        "tier": "large",
        "size_range_mb": [
          10,
          100
        ],
        "strategy": "streaming_chunks",
        "preview_rows": 10000,
        "chunk_size": 50000,
        "memory_cap_mb": 50
      },
      {
        "tier": "very_large",
        "size_range_mb": [
          100,
          1000
        ],
        "strategy": "streaming_with_limits",
        "preview_rows": 5000,
        "chunk_size": 50000,
        "memory_cap_mb": 100
      },
      {
        "tier": "massive",
        "size_range_mb": [
          1000,
          null
        ],
        "strategy": "partitioned_streaming",
        "preview_rows": 1000,
        "chunk_size": 100000,
        "memory_cap_mb": 200
      }
    ],
    "streaming_implementation": {
      "description": "Streaming uses Polars LazyFrame with scan_* functions",
      "pattern": "async_generator",
      "example_flow": [
        "1. Check file size against threshold",
        "2. Create LazyFrame with scan_csv/scan_ndjson",
        "3. Collect in batches using streaming=True",
        "4. Yield each batch with StreamChunk metadata",
        "5. Check cancel token between batches",
        "6. Create checkpoint after each successful batch"
      ],
      "polars_config": {
        "streaming_chunk_size": "Set via pl.Config.set_streaming_chunk_size()",
        "lazy_functions": [
          "scan_csv",
          "scan_ndjson",
          "scan_parquet"
        ]
      }
    },
    "memory_manager": {
      "location": "apps/data_aggregator/backend/core/memory_manager.py",
      "responsibilities": [
        "Track current memory usage",
        "Enforce memory limits",
        "Trigger garbage collection",
        "Spill to disk if needed"
      ]
    },
    "progress_tracking": {
      "websocket_endpoint": "/ws/dat/runs/{run_id}/progress",
      "update_frequency": "After each chunk or every 5 seconds",
      "payload": {
        "stage_id": "string",
        "progress_pct": "float",
        "rows_processed": "int",
        "chunks_completed": "int",
        "estimated_remaining_ms": "int | null"
      }
    }
  },
  "api_endpoints": [
    {
      "method": "GET",
      "path": "/api/dat/runs/{run_id}/stages/parse/progress",
      "description": "Get current parse progress",
      "response": {
        "progress_pct": "float",
        "rows_processed": "int",
        "current_file": "string",
        "streaming_mode": "bool"
      }
    },
    {
      "method": "POST",
      "path": "/api/dat/runs/{run_id}/stages/parse/cancel",
      "description": "Cancel streaming operation",
      "response": "CancellationResult"
    }
  ],
  "frontend_considerations": {
    "virtualized_table": {
      "library": "TanStack Virtual + shadcn/ui DataTable",
      "purpose": "Render large preview datasets efficiently",
      "max_rows_in_dom": 100,
      "row_height": 35
    },
    "progress_display": {
      "components": [
        "ProgressBar",
        "ETA display",
        "Cancel button"
      ],
      "update_via": "WebSocket connection"
    },
    "preview_sampling": {
      "description": "For large files, preview shows sampled data",
      "indicator": "Show 'Showing X of Y rows (sampled)' badge"
    }
  },
  "validation_commands": [
    "pytest tests/dat/test_streaming.py -v",
    "pytest tests/dat/test_large_files.py -v --runslow"
  ],
  "test_fixtures": [
    {
      "name": "small_csv",
      "size": "50KB",
      "rows": 1000
    },
    {
      "name": "medium_csv",
      "size": "5MB",
      "rows": 100000
    },
    {
      "name": "large_csv",
      "size": "50MB",
      "rows": 1000000
    },
    {
      "name": "very_large_csv",
      "size": "500MB",
      "rows": 10000000,
      "skip_ci": true
    }
  ],
  "acceptance_criteria_summary": [
    "Files > 10MB automatically use streaming mode",
    "Memory usage stays within 200MB cap",
    "Progress updates every 5 seconds or per chunk",
    "Cancellation preserves completed chunks",
    "Schema probe < 5 seconds for any file size",
    "Preview loads in < 2 seconds using sampling"
  ],
  "related_specs": [
    "SPEC-DAT-0003_Adapter-Interface-Registry",
    "SPEC-DAT-0015_Cancellation-Cleanup"
  ],
  "tags": [
    "dat",
    "streaming",
    "large-files",
    "performance",
    "memory"
  ]
}