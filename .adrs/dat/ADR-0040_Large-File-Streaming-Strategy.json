{
  "schema_type": "adr",
  "id": "ADR-0040_Large-File-Streaming-Strategy",
  "title": "Large File Streaming Strategy with 10MB Threshold",
  "status": "accepted",
  "date": "2025-12-28",
  "review_date": "2026-06-28",
  "deciders": "Mycahya Eggleston",
  "scope": "subsystem:DAT",
  "provenance": [
    {
      "at": "2025-12-28",
      "by": "Mycahya Eggleston",
      "note": "Initial decision for handling files from 1KB to multi-GB scale"
    }
  ],
  "context": "DAT must handle files ranging from 1KB to multi-GB while maintaining responsive UI and preventing memory exhaustion. Different file sizes require different processing strategies. Small files can be loaded entirely into memory for fast operations, while large files require streaming to avoid memory issues. A clear threshold and processing strategy for each tier ensures predictable behavior and optimal resource usage.",
  "decision_primary": "DAT uses a tiered file processing strategy with 10MB as the primary threshold between eager loading and streaming modes. Files <= 10MB are loaded entirely into memory for full interactivity. Files > 10MB use Polars LazyFrame streaming with configurable chunk sizes. All adapters MUST implement both read_dataframe (eager) and stream_dataframe (streaming) methods.",
  "decision_details": {
    "approach": "Tiered processing strategy based on file size with automatic mode selection. UI adapts to show appropriate feedback for each tier.",
    "file_size_tiers": [
      {
        "tier": "small",
        "size_range": "< 100KB",
        "strategy": "eager_load",
        "memory_usage": "~10x file size",
        "ui_behavior": "Instant preview, full interactivity",
        "preview_rows": "all"
      },
      {
        "tier": "medium",
        "size_range": "100KB - 10MB",
        "strategy": "eager_load_with_progress",
        "memory_usage": "~5x file size",
        "ui_behavior": "Progress bar during load, full preview available",
        "preview_rows": "all"
      },
      {
        "tier": "large",
        "size_range": "10MB - 100MB",
        "strategy": "streaming_chunks",
        "memory_usage": "~50MB cap",
        "ui_behavior": "Chunked progress, sampled preview (10k rows)",
        "preview_rows": 10000,
        "chunk_size": 50000
      },
      {
        "tier": "very_large",
        "size_range": "100MB - 1GB",
        "strategy": "streaming_with_limits",
        "memory_usage": "~100MB cap",
        "ui_behavior": "Background processing, WebSocket updates, sampled preview",
        "preview_rows": 5000,
        "chunk_size": 50000
      },
      {
        "tier": "massive",
        "size_range": "> 1GB",
        "strategy": "partitioned_streaming",
        "memory_usage": "~200MB cap",
        "ui_behavior": "Job queue, checkpointing, resume support",
        "preview_rows": 1000,
        "chunk_size": 100000
      }
    ],
    "streaming_threshold_mb": 10,
    "constraints": [
      "All adapters MUST implement stream_dataframe for files > 10MB",
      "Schema probing MUST complete in < 5 seconds regardless of file size",
      "Preview MUST show within 2 seconds using sampling for large files",
      "Memory usage MUST NOT exceed configured max_memory_mb",
      "Streaming operations MUST support cancellation with checkpoint preservation",
      "Progress updates MUST be emitted at least every 5 seconds during processing"
    ],
    "implementation_specs": [
      "SPEC-DAT-0004_Large-File-Streaming",
      "shared.contracts.dat.adapter.StreamOptions"
    ]
  },
  "consequences": [
    "Files up to 10MB have instant, full interactivity",
    "Large files process without memory exhaustion",
    "UI remains responsive during large file operations",
    "Checkpoint-based resume for very large file failures",
    "Some complexity in adapter implementations for dual-mode support",
    "Preview is sampled for large files (not all rows visible)"
  ],
  "alternatives_considered": [
    {
      "name": "Always Eager Load",
      "pros": "Simpler implementation, full interactivity",
      "cons": "Memory exhaustion for large files, crashes on multi-GB files",
      "rejected_reason": "Platform must handle multi-GB files for real-world use cases"
    },
    {
      "name": "Always Stream",
      "pros": "Consistent behavior, memory-safe",
      "cons": "Slow for small files, unnecessary overhead",
      "rejected_reason": "Degrades UX for common small file use cases"
    },
    {
      "name": "1MB Threshold",
      "pros": "More conservative memory usage",
      "cons": "Too aggressive - most CSV files are 1-10MB and benefit from eager loading",
      "rejected_reason": "10MB threshold balances memory safety with usability"
    },
    {
      "name": "100MB Threshold",
      "pros": "Maximum interactivity range",
      "cons": "Risk of memory issues on lower-spec machines",
      "rejected_reason": "10MB is safer for diverse deployment environments"
    }
  ],
  "tradeoffs": "We accept dual-mode adapter complexity for optimal UX across file sizes. We accept sampled previews for large files to maintain responsiveness.",
  "guardrails": [
    {
      "rule": "Streaming Threshold: Files > 10MB MUST use streaming mode",
      "enforcement": "Adapter registry enforces streaming for files exceeding threshold",
      "scope": "subsystem:DAT",
      "id": "dat-streaming-threshold"
    },
    {
      "rule": "Memory Cap: Operations MUST NOT exceed max_memory_mb configuration",
      "enforcement": "Memory manager monitors and enforces limits during processing",
      "scope": "subsystem:DAT",
      "id": "dat-memory-cap"
    },
    {
      "rule": "Schema Probe Speed: Schema probing MUST complete in < 5 seconds",
      "enforcement": "Probe operations use sample-based inference with timeout",
      "scope": "subsystem:DAT",
      "id": "dat-probe-speed"
    }
  ],
  "cross_cutting_guardrails": [
    "Cancel Behavior: Streaming cancellation preserves checkpoints (see ADR-0013)",
    "Path Safety: All file paths must be relative (see ADR-0017#path-safety)",
    "Concurrency: Streaming uses spawn-safe executor (see ADR-0012)"
  ],
  "references": [
    "ADR-0011_Profile-Driven-Extraction-and-Adapters: Adapter pattern this extends",
    "ADR-0013_Cancellation-Semantics-Parse-Export: Checkpoint preservation on cancel",
    "ADR-0012_Cross-Platform-Concurrency: Spawn-safe concurrency for streaming"
  ],
  "tags": [
    "dat",
    "streaming",
    "large-files",
    "memory-management",
    "performance"
  ],
  "affected_components": [
    "shared/contracts/dat/adapter.py",
    "apps/data_aggregator/backend/adapters/",
    "apps/data_aggregator/backend/core/memory_manager.py",
    "apps/data_aggregator/frontend/src/components/stages/ParsePanel.tsx"
  ]
}
